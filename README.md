### Gradient Descent Optimizers Comparison


This project explores different gradient descent optimization techniques on the MNIST dataset. We implement and compare key optimizers — Stochastic Gradient Descent (SGD), Momentum, RMSprop, and Adam — to gain insights into their effectiveness across various performance metrics such as accuracy, training time, memory usage, and robustness to noise.
